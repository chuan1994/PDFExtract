package testCases.PHDThesisTests;

import static org.junit.Assert.*;
import main.MetadataStorer;

import org.junit.Test;

public class PHDTests1 {
	
	@Test
	public void test1() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Healthcare4Life: A Ubiquitous Patient-Centric Telehealth System");
		String[] author = {"Jaspaljeet Singh Ranjit Singh"};
		tester.setAuthors(author);
		tester.setSupervisors("Dr. Burkhard Wünsche and Dr. Christof Lutteroth");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Over the past decade, healthcare costs and especially senior healthcare costs have increased rapidly and are making traditional healthcare concepts unaffordable for many developed countries. Telehealth is thought to be a solution for effective senior healthcare. However, existing telehealth systems are focused on treating diseases instead of preventing them, suffer from high initial costs, and lack extensibility. They also do not address the social and psychological needs of the patients. In this thesis, a novel patient-centric telehealth system for seniors called Healthcare4Life is presented and evaluated. The system provides access to a variety of health-related applications, which encourage positive lifestyle changes. Users are able to locate other users or patients suffering from similar diseases, enabling them to share experiences, motivate each other, and engage in health-related activities, such as tracking physical activities and vital signs. Similar to Facebook, the system has an open architecture that enables third-party providers to add new content and functionalities. User requirements of Healthcare4Life were elicited via interviews conducted with potential users. Web 2.0 technologies are evaluated with regard to their suitability for the development of extendable telehealth systems with social networking capabilities. Based on these findings, a system prototype is developed and evaluated. A formative evaluation approach is presented, confirming that seniors are satisfied with the overall concept and usability. A summative evaluation conducted with support from senior community centres with a larger sample size is presented, demonstrating the overall feasibility and acceptability of the system. The results demonstrate that the combination of telehealth functionalities with social components and user-generated content is a promising way to enable users to proactively manage and improve their health. However, the results suggest that in order to be useful in practice the system needs a large user base and must offer services making it attractive to application developers and content providers. Web standards, such as OpenSocial, and a content management system, such as Drupal, integrate successfully and provide a foundation to convert ordinary systems into open-ended platforms. The evaluation indicates that the target user population, seniors, were keen to leverage Healthcare4Life for their healthcare, provided there is a range of health applications tailored towards individual needs. Social networking functionalities are desired, but should have a clear purpose such as social games or exchanging information, rather than broadcasting emotions and opinions. Results further suggest that web-based telehealth systems hold great potential to positively change the attitude of users that health is not controlled by others such as health professionals, but consumers have the power to positively affect their well-being.");
		
		fail("Not yet implemented");
	}
	
	@Test
	public void test2() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Real Time Stereo on GPU with Application to Precision 3D Tracking");
		String[] author = {"Ratheesh Kalarot"};
		tester.setAuthors(author);
		tester.setSupervisors("Dr. John Morris and Dr. Georgy Gimel’farb");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Real-time high resolution 3D scene maps are essential for many computer vision-based applications like autonomous vehicle navigation and collision avoidance. However, stereo vision is an ill-posed and computationally demanding problem which requires both efficient regularisation and hardware acceleration to generate accurate depth maps from image pairs with large disparity ranges. While dozens of stereo algorithms exist in the literature, not many of them are adaptable to modern GPU architectures. Several potential realtime stereo algorithms were implemented on GPUs and their speed and accuracy measured. Symmetric Dynamic Programming Stereo was found to be the best candidate, providing good accuracy at the target 30 frame per second rates. Its GPU solution was further optimised for high definition, low latency GPU performance and analysed in detail. A comparison with a known similar implementation on FPGA highlighted relative strengths and limitations of different hardware architectures. This thesis presents the highest throughput GPU based stereo system reported in literature so far: it evaluates more than 15 billion disparities per second on an inexpensive commercial GPU. This system was further extended with a multi-resolution computation framework for faster processing. This approach not only reduces the overall computation demands and thus increases achievable frame rates, but also incorporates scene constraints and inter scan-line consistency directly into the disparity calculation in order to improve the matching performance. The thesis also applies the developed stereo system to a challenging problem of real-time object detection and tracking. An observed scene is described with 3D point clouds provided by the system, thus trading inevitably some accuracy for speed. Also, noisy depth measurements and artefacts hinder tracking accuracy. To reduce artefacts and segment foreground objects to be tracked, a joint depth and colour filter was used. Then a fast clustering procedure based upon a set of simple volume rules identified the candidate objects and an opportunistic tagging system tracked objects through occlusions. Kalman filtering with distance-specific error covariance was implemented to accurately predict positions of objects in the next frame. Experiments with many synthetic and real-world video sequences confirmed the accuracy in tracking multiple people in various environments.");		
		fail("Not yet implemented");
	}
	
	@Test
	public void test3() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Using Buddhist Insights to Analyse the Cause of System Project Failures");
		String[] author = {"Pita Jarupunphol"};
		tester.setAuthors(author);
		tester.setSupervisors("Professor Clark Thomborson and Associate Professor Timothy Teo");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("This thesis hypothesises that Buddhist insights can be useful in systems engineering and related disciplines. The thesis commences by arguing that upadana, or a condition of attachment can be considered as the root cause of problems in systems engineering. Definitions of attachment from psychology are also discussed in connection to the condition of attachment in Buddhism. Similarities and differences are pointed out. A number of theories, approaches, and frameworks in systems engineering are reviewed and discussed in connection to the condition of attachment. A case study of Secure Electronic Transaction (SET) failures is used to illustrate that the condition of attachment underlies system project failures. This thesis provides the background behind the emergence of SET, a technical overview of it, issues of SET, attempted solutions, and emerging e-payment systems after SET failures. Central to the thesis is the Dhammic framework, a novel theoretical framework grounded in the central tenet of Buddhism that represents states of mind from a Buddhist perspective. This thesis formalises states of mind from Buddhist Dhamma using a formal method in order to avoid ambiguity. The framework is based on chosen doctrines from Buddhist Dhamma and a formal method to logically represent how the reality of a phenomenon is rooted in an individual’s perceptions. Psychological constructs underlying how a condition of attachment is derived, where a condition of attachment is situated, and why a condition of attachment can be undesirable are pointed out. The Dhammic framework can be considered as an iconoclastic framework that is not utilised in existing analytical frameworks for understanding system project failures in the literature, but worth considering in systems engineering. Dhammic TAM (DTAM), which is based on TAM (Technology Acceptance Model) extended by the condition of attachment in Buddhism, is tested using empirical data in order to investigate if the condition of attachment should be considered as a critical construct for predicting behavioural intentions toward using a system. The methodology and the experimental results with reference to DTAM are also discussed in the thesis. The results also confirm whether or not the condition of attachment in Buddhism can be useful in systems engineering and related disciplines. The thesis also discusses Majjhima Patipada or the Middle Path (MP), which is an insight for dealing with conflicts in Buddhism. Different MP theories are discussed in reference to modern conflict resolution theories in psychology. Given that the condition of attachment plays an important role in system acceptance, the thesis recommends Buddhist MP systems engineering as an appropriate path for avoding system project failures.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test4() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Leveraging Constraint-based Layout for User Interface Customization");
		String[] author = {"Clemens Zeidler"};
		tester.setAuthors(author);
		tester.setSupervisors("Christof Lutteroth and Gerald Weber");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Science");
		tester.setAbstractx("In this thesis the usage of constraint-based layout in the field of user interface (UI) customization is explored. The constraint-based layout model is very powerful and can be used for many different kinds of layouts. However, it is also more complex than most other layout models, which makes it challenging for users to create sound layouts, i.e., layouts that are solvable and do not allow layout items to overlap. To leverage the constraintbased layout model for UI customization, we present methods that enable users to create and edit constraint-based layouts in a sound manner. To motivate why the constraint-based layout model is used in this work, it is compared to other layout models. In a user evaluation the usability of the constraint-based layout model was compared to the grid-bag layout model, which is also very powerful and likely the most commonly used model. Another evaluation investigated the aesthetic aspects of how available space in a layout should be best distributed among widgets. The first system for UI customization that we analyzed is Stack & Tile. It allows the user to stack and tile windows from a traditional desktop system into groups. Window groups are specified using the constraint-based layout model. A user evaluation showed that Stack & Tile substantially improves the work with multiple windows. Furthermore, we explored in a web survey how and if Stack & Tile is actually used by real users. The second system for UI customization targets the editing of constraint-based layouts at application runtime. A set of edit operations is developed that makes it easy to edit constraint-based layout in a sound manner. To evaluate these edit operations, we implemented them in a graphical user interface (GUI) builder, the Auckland Layout Editor (ALE). In a user study participants performed significantly faster for layout creation and layout editing tasks compared to other layout builders. Another contribution of this thesis is a new way to describe constraint-based layouts using a formal algebraic description for layout specifications. This algebra can be used to describe sound layout operations formally, i.e., operations that keep a layout solvable and non-overlapping. The edit operations used in Stack & Tile and ALE are then mapped to these algebraic operations. To investigate if and where UI customization is useful, a user evaluation was conducted.This study covered layout as well as functional customization. For this a functional customization prototype was developed that allows changing the functionality and behavior of an application. The evaluation showed that users are keen to customize applications to their needs");
		fail("Not yet implemented");
	}
	
	@Test
	public void test5() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Reducing Screen Occlusions on Small Displays");
		String[] author = {"Paul Schmieder"};
		tester.setAuthors(author);
		tester.setSupervisors("Beryl, John and Andrew");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp(null);
		tester.setAbstractx("Information occlusion on small displays is a familiar problem affecting interaction on multiple levels. This research investigates solutions to two causes of screen occlusion: first, temporal and spatial occlusion caused by magnification strategies such as zooming and overview + detail interfaces and, second, the physical occlusion caused by fingers interacting with the display. The findings from each area considerably reduce screen occlusion on small displays. To overcome occlusions caused by magnification interfaces, distortion-based strategies are used and applied to the sketching domain for rapid content creation and editing. The use of distortion-based magnification strategies has been limited to viewing and selecting information; usability problems, such as the comprehension difficulties caused by the visual transformations employed, have not been solved. A novel combination of sketching is used as the context to investigate information creation and editing using distortion techniques. Sketching allows to identify the requirements for content creation and editing, and also addresses problems shared with content viewing and selecting. To investigate the requirements for magnified sketching, first the problems existing distortion interfaces have when used for sketching are identified. Next, solutions to address the identified problems are developed and tested iteratively before concluding with a comparison of the improved distortion interface and other magnification strategies. The results of this final comparison of the distortion, zooming and overview + detail interfaces demonstrate the improved distortion interface’s efficacy and that it is the participants’ preferred interface for magnified content editing and creation. To overcome finger-based occlusion, the use of 3D gestures captured by a camera is investigated, thus moving the interaction space away from the display. When entering data on a touchscreen using the fingers, the information below the finger is covered and thus hidden from the user. An input mechanism is developed and evaluated which uses 3D gestures captured by the front-facing camera. As this input metaphor is novel, the first step was to investigate the requirements of such an interaction on the device as well as on the user. To this end, a set of gestures of varying demands on the user was devised and recognition algorithms for these gestures developed. The resultant gesture interface was tested on users. The results show the potential of the new interaction mechanism and provide further insight into successful and robust recognition strategies.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test6() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Evaluating and Improving SHIM6 and MPTCP: Two Solutions for IPv6 Multihoming");
		String[] author = {"Habib Naderi"};
		tester.setAuthors(author);
		tester.setSupervisors("Associate Professor Nevil Brownlee, Professor Brian Carpenter and Dr Ulrich Speidel");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Site multihoming refers to a situation when a site has access to the Internet through more than one Internet service provider. In the current Internet, multihoming is achieved by using Border Gateway Protocol. A multihomed site acquires its provider independent or provider aggregatable address block and then announces this address block through all its service providers to the Internet’s global routers. This solution works for IPv4, but scalability is an issue for IPv6. High demand for multihoming and the huge address space provided by IPv6 requires a solution which is able to scale well. A wide range of solutions have been proposed for IPv6 multihoming during past years. These solutions can be put into three categories: Routing approaches, MiddleBox approaches and Host-centric approaches. The first part of our research focuses on an analysis, from a deployability viewpoint, of seven proposed solutions which are still under investigation by researchers and Internet experts. The analysis shows that there is no perfect solution yet and all proposed solutions still need to be improved. However, host-centric solutions are more attractive from the viewpoint of deployability since they do not need any change in the Internet routing system. SHIM6 and MultiPath TCP (MPTCP) are two incrementally deployable solutions belonging to this category. Failure detection and recovery without breaking active communications is one of the main requirements for every multihoming solution. MPTCP and SHIM6 are both able to provide this functionality. SHIM6 is equipped with a specific failure detection and recovery protocol (REAchability Protocol) and MPTCP employs a central congestion control mechanism (SEMICOUPLED Congestion Control Algorithm) for this purpose. These protocols are not yet widely deployed on the Internet, so the efficiency of these protocols is still an open research area. The second part of the work presented in this thesis focuses on this research area by conducting a series of real-world and simulation experiments for SHIM6 and simulation experiments for MPTCP which have been designed to investigate the efficiency of REAP and SEMICOUPLED. Traffic Engineering is one of the major weak points of SHIM6. Since the solution is implemented in end-hosts, having a site-aware traffic engineering mechanism is challenging. To address this issue, we propose a dynamic traffic engineering mechanism which offers a rich set of traffic engineering features for SHIM6-enabled sites.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test7() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Clustering, Swarms and Recommender Systems");
		String[] author = {"Shafiq Alam"};
		tester.setAuthors(author);
		tester.setSupervisors("Prof. Gillian Dobbie");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Knowledge Discovery and Data mining (KDD) helps us understand some characteristics of data in large repositories by passing the data through different operations such as data selection, preprocessing, transformation, data mining and post processing. Data mining, which is the core of KDD, extracts informative patterns such as clusters of relevant data, classification and association rules, sequential patterns and prediction models for different types of data such as textual, audio-visual, and microarray data. Recently a huge increase in the use of Swarm Intelligence (SI) based optimization techniques for KDD has been observed. Particle Swarm Optimization (PSO) is one of the most highly cited SI techniques for KDD because it has the flexibility, simplicity, and extendibility to be used for different data mining tasks. In this thesis, we exploited the potential of PSO for data clustering and analyzed the performance of PSO in different application areas. We extended the basic PSO model into Evolutionary PSO clustering (EPSO-clustering) and Hierarchical PSO clustering (HPSOclustering). The later technique clusters the data in a hierarchy of clusters, combining the benefits of hierarchical and partitional clustering. It provides a clustering solution at different levels of granularity. We tested our approach on benchmark classification data and extended the approach to be used for clustering web usage data. Besides good performance on classification data, HPSO-clustering has outperformed traditional clustering techniques on web usage data. Web usage data is different from traditional data, therefore, before clustering, the data needs to be passed through sophisticated analysis and preprocessing phases. One of the problems with web usage data is the poor quality of the web usage data. It is sparse, noisy and approximately 60% to 80% of the web usage data does not contribute to the pattern mining process. To clean and prepare the web usage data for clustering, we proposed an analysis and preprocessing model. The proposed model divides the entire cleaning process into sub phases such as log-based analysis, subject-based analysis, general preprocessing, and algorithm specific preprocessing. We presented the results of each phase with the help of two experimental studies, one based on data from the NASA website, and the other on data from the Department of Computer Science, University of Auckland web server. Another problem with web usage data is that the noise in the data is not from genuine web users. A number of known and unknown web crawlers browse the web pages. Alongside the genuine web users, the activity of the web crawlers is also recorded in the web log. The behavior of the web crawlers is different from humans so their sessions need to be identified and analyzed separately. To tackle this problem, we extended our HPSO-clustering approach to use it for cluster-based outlier detection. Besides verifying the technique for identifying outliers in numeric data, we used it to detect web crawlers in web usage data. We selected recommender systems to demonstrate an application of the patterns generated from web usage data based on our proposed analysis, clustering, and outlier detection model. Recommender systems have emerged as one of the major applications of the patterns extracted from the behavior of web users and resources. Web-based recommender systems are tools that assist users find a particular web resource based on the history of the web user or the usage of that resource. Building web based implicit recommender systems is not a trivial task due to the amount of data and its poor quality. After subsequent preprocessing and clustering phases, we generated recommendations for different scenarios of an active user. We present detailed results of accuracy of different recommendations which verify the capability of our proposed technique. In our research, besides proposing an analysis and preprocessing model, we researched PSO as a method for performing efficient and flexible clustering and a method for detecting outliers from web usage data. All these issues are crucial for building efficient and accurate implicit data based recommender systems.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test8() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Improving the Usability of Real-Time Passive Stereo Sensors");
		String[] author = {"Muhammad Tariq Ali Khan"};
		tester.setAuthors(author);
		tester.setSupervisors("Associate Professor John Morris and Dr Morteza Biglari-Abhari");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp(null);
		tester.setAbstractx("here is an increasing industrial demand for robust real-time intelligent vision systems. High resolution stereo can play an important role but the high computational complexity for generating a depth map is the main concern. Hardware acceleration (using GPU or FPGA) can help satisfy real-time requirements. Symmetric Dynamic Programming Stereo (SDPS) is the highest throughput algorithm available on hardware (both in GPU and FPGA). However, the depth maps are generally noisy which reduces its usability. Therefore, the target here is to design and propose fast post-processing algorithms for the data generated by SDPS hardware which will improve its usability and will assist high level vision tasks. This thesis makes three main contributions. The first one is the fast depth contour generation algorithm. Depth contours represent object shapes which is the key feature for object modelling, detection, pose estimation and recognition. The second contribution is the Points-of-Interest detection algorithm; it is fast and robust to depth noise due to its use of left and right image pair. Both contour generation and Points-of-Interest detection take one pass which reduces the search space and then uses the reduced search space for subsequent processing. The third contribution are the two depth contour refinement algorithms. To improve depth contours, conventionally depth maps are refined before generating contours which can remove some fine details present in depth map. Two novel contour refinement algorithms are presented in this thesis which improve depth contours and reduce the number of streaks in the contour while preserving fine detail. ");
		fail("Not yet implemented");
	}
	
	@Test
	public void test9() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("P Systems as Formal Models for Distributed Algorithms");
		String[] author = {"Huiling Wu"};
		tester.setAuthors(author);
		tester.setSupervisors("Dr. Radu Nicolescu");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("A P system is a parallel and distributed computational model inspired by the structure and interactions of living cells [55]. Chapter 1 first relates mainstream concepts of distributed computing to distributed features proposed for P systems. Chapter 2 introduces xP systems, which are extended versions of simple P systems proposed in our joint work [51, 3, 52, 22, 50]. This thesis investigates the adequacy of xP systems for modelling fundamental synchronous and asynchronous distributed algorithms and aims to construct xP specifications, i.e. directly executable formal specifications of algorithms in xP systems, which: (1) achieve the same runtime complexities as corresponding distributed algorithms and (2) are comparable in program size with high-level pseudocodes of corresponding distributed algorithms. Chapter 3 presents xP specifications of several fundamental traversal algorithms: Echo, distributed depth-first search (DFS), distributed breadth-first search (BFS) and neighbour discovery algorithms, based on our joint work [3]. As new contributions in this thesis, we∗ address the common problem in distributed algorithms, termination detection problem, and provide xP specifications of several well-known termination detection algorithms and their applications. Chapter 4 discusses a series of distributed synchronous DFS and BFS-based edgeand node-disjoint paths algorithms. Consider a digraph with n nodes and m arcs, where f is the maximum number of disjoint paths and d is the outdegree of the source node. Dinneen et al.’s [18] algorithms based on the classical DFS run in O(mf); using Cidon’s DFS and Theorem 4.5, our improved algorithms [52] run in O(nf); using a different idea, our two other DFS-based algorithms [22] run in O(nd) and O(nf). The first BFS-based P system solutions in our joint work [51, 52] run in O(nf); an improved version as my own work [65] also runs in O(nf). Following my own work [64], Chapter 5 presents a solution for one of the most challenging distributed computing problems: minimum spanning tree (MST) problem. We discuss the SynchGHS algorithm [42] and our synchronisation barriers. Given a weighted graph with n nodes, our xP solution runs in O(n log n) and it is the first MST solution in P systems. ");
		fail("Not yet implemented");
	}
	
	@Test
	public void test10() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("High-order Markov-Gibbs Random Field Models for Texture Recognition");
		String[] author = {"Ni Liu"};
		tester.setAuthors(author);
		tester.setSupervisors(" Professor Georgy Gimel’farb");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Texture analysis plays a fundamental role in computer vision applications such as image understanding, object recognition, scene segmentation and so on. This thesis investigates novel types of high-order Markov-Gibbs random field (MGRF) models of images in application to texture recognition and retrieval. The goal is to discriminate a texture class represented by a single training (query) sample from other classes. Frequent in practice spatially variant perceptive (contrast/offset) deviations that preserve the image appearance hinder the recognition by signal co-occurrence statistics. Contrast/offsetinvariant descriptors of ordinal signal relations, such as local binary or ternary patterns (LBP/LTPs), are efficient and popular means to overcome such drawback. The thesis explores the use of ordinal relations instead of more conventional signal co-occurrences or responses of filter banks used in today’s MGRFs. Textured images are considered as samples from high-order ordinal models, which allow for learning, rather than prescribing characteristic shapes, sizes, and numbers of these patterns for texture recognition and retrieval. Approximate analytical estimates of the model parameters guide the selection of characteristic LBP/LTPs of a given order. The higher-order patterns are learned on the basis of the already found lower-order ones. The thesis introduces first the MGRFs based on complete ordinal relations of the signals. Because the cardinality of their set grows quickly, only up to the 4th–5th-order ordinal MGRFs remain feasible. Partial ordinal LBP/LTP-based relations have less limitations, so that higher-order models can be learned (up to 12th-order in this thesis) for representing textures. Comparative experiments on six databases confirmed that classifiers using multiple learned LTPs from the 8th–12th-order consistently outperform more conventional ones with prescribed fixed-shape LBP/LTPs or other local filters. The proposed learned models are mostly good enough to describe the textures. However, heuristic rules for feasible learning of the high-order models sometimes miss characteristic short-range dependencies. Adding the nearest-neighbour circular LTPs to the learned characteristic high-order LTPs overcomes this drawback and further improves the performance over the previous models. A learnable 5th-order LTP-based MGRF was also applied as a visual appearance prior for segmenting kidney images for automated medical diagnostics. Experiments confirmed that the proposed model outperforms two more conventional counterparts both qualitatively and quantitatively.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test11() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Automated Compliance Audit Processes for Building Information Models ");
		String[] author = {"Johannes Alwi Wibisono Dimyadi"};
		tester.setAuthors(author);
		tester.setSupervisors("Prof. Robert Amor");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science");
		tester.setAbstractx("Every architectural and building engineering design in the AEC/FM (Architectural, Engineering, Construction, and Facility Management) domain is subject to conformance with a diverse range of statutory requirements and standards before the official consent for construction may be granted by the relevant authority. There is an enormous amount of information to process and share among the stakeholders for this to occur. Historically, building information has been exchanged in the domain using paper-based drawings and written specifications. Auditing a building design for compliance, for example, has been a manual undertaking against a voluminous corpus of paperbased legal texts that are written in natural language intended for human interpretation. This paper-based practice is resource intensive, inefficient, and error-prone, which has sometimes resulted in costly remediations of problem buildings and even loss of lives. It is also a factor for declining productivity in the domain. There are now methods of sharing digital building information via building information modelling (BIM), which promotes better collaboration and provides more opportunities for automation in the compliant building design process. However, there has not been a successful standard automated compliance audit implementation to date despite considerable research effort over the years. This research has explored a practical approach where human designers retain their natural role of making compliant design decisions. Machines are only given the task of doing what they do best, which is to execute repetitive instructions and procedures consistently and accurately. The findings of the research suggest that this human-machine partnership approach allows conventional compliant design procedures (CDPs) to be formalised as executable workflows to guide the automated audit of BIM models against regulatory knowledge models (RKMs). An important criterion in the approach is that CDP and RKM representations must be manageable by the domain experts equipped with only basic computing skills. A software framework (ARCABIM) has been developed to illustrate the approach using a test building model for evaluation. The potential of interfacing with external computational tools required to audit against qualitative performance-based criteria is described. A fire compliance model (FCM) representing a BIM model view for the compliant fire safety design of buildings has been developed and used in a case study to demonstrate the capabilities of the approach in conjunction with an RKM representing a performance-based fire safety design method of the New Zealand Building Code.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test12() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Constraint Solving for User Interface Layout Problems");
		String[] author = {"Noreen Jamil"};
		tester.setAuthors(author);
		tester.setSupervisors("Dr. Christof Lutteroth and Dr. Gerald Weber");
		tester.setDegree("DOCTOR OF PHILOSOPHY");
		tester.setDegreeDiscp("SCIENCE");
		tester.setAbstractx("This thesis proposes efficient and robust algorithms for solving linear constraints problems for graphical user interface (GUI) layout. Constraints have been recognised as a powerful method for the specification of GUI layouts for a long time. Most constraint problems encountered in this area are sparse, i.e. most linear coefficients in the corresponding coefficient matrix are zero. The numerical methods that have been proposed for solving layout problems so far are mainly direct methods. The focus of this research is on investigating iterative methods for solving these problems efficiently. The algorithms developed in this thesis are compared to well-known direct and linear programming algorithms. The performance and convergence of the proposed and existing algorithms were evaluated empirically using randomly generated UI layout specifications of various sizes. Successive Over-Relaxation (SOR) is one of the most commonly used iterative methods for solving linear problems, and this was the first algorithm investigated in this thesis. In contrast to direct methods such as Gauss-elimination or QR-factorization, SOR is particularly efficient for problems with sparse matrices, as they appear in GUI layout specifications. However, SOR as described in the literature has its limitations: it works only with square matrices and does not support soft constraints, which makes it inapplicable to UI layout problems. This research extends SOR so that it can be used to solve non-square matrices and soft constraints. Furthermore, different optimizations of SOR were investigated to speed up its convergence. First, we propose a metric to measure the optimality of a constraint sequence and then a Simulated Annealing based algorithm, that optimizes the order in which constraints are solved. Second, we propose Constraint-Wise Under-relaxation (CWU), a technique in which the relaxation parameters of individual constraints are adjusted during solving. Third, we investigate the use of a warm start strategy, which reuses the solution of a previous layout to warm start the solving of a new layout, taking into account that most layout changes during runtime are small. Another contribution of this thesis is an investigation of the Kaczmarz method – an iterative orthogonal projection algorithm – for solving GUI layout problems. This algorithm is more efficient than the SOR algorithm and its convergence is guaranteed. However, to make it applicable to GUI problems some extensions were necessary, such as support for soft constraints. We also investigate some approaches for least-squares solving and optimization in this context. ");
		fail("Not yet implemented");
	}
	
	@Test
	public void test13() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Hypertext Navigation with an Eye Gaze Tracker");
		String[] author = {"A. Moiz Penkar"};
		tester.setAuthors(author);
		tester.setSupervisors("Gerald and Christof");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Science");
		tester.setAbstractx("Gaze tracking is considered to be a promising component of future user interfaces. It is faster than other pointing methods and represents our thoughts and intentions in a natural manner. Gaze tracking equipment is also becoming more affordable whilst improving in tracking performance and usability. However, there are inherent difficulties in using gaze tracking as an input device. The first difficulty is the lack of accuracy due to the involuntary eye movements, limited pointing resolution of eyes and gaze tracker equipment error. The second difficulty is the inability to differentiate between intentional and unintentional gaze. As eyes are primarily sensory organs, even the intentional gaze may be for perception only and not for performing any action. The different techniques developed to perform actions by gaze attempt to avoid unintended actions (aka inadvertent clicks). Compensating for the inaccuracy and inadvertent clicks introduces complexity in the interaction, making it slow and tiring. Consequently, existing techniques of performing actions with gaze are only considered feasible for users with limited accessibility. This thesis investigates the potential of universal gaze controlled applications by designing and evaluating gaze control techniques that are fast, easy to use and natural, while also avoiding inadvertent clicks. The behavior of gaze is quite task-specific, therefore we focus on a single popular task: hypertext navigation. First, we evaluated design parameters of buttons activated by dwell. Our main findings indicate that placing the content outside the buttons, completely avoids inadvertent clicks, and that longer dwell time results in less accuracy. Second, we designed and evaluated four techniques for performing actions by gaze in the context of hypertext navigation. Their performances were compared to the mouse using quantitative and qualitative measures. The Multiple Confirm turned out to be the best gaze based alternative, being as accurate as the mouse but much slower. Finally, the TouchEye technique of performing actions by gaze was developed as a significant improvement over Multiple Confirm. TouchEye associates hyperlinks on a web page with confirm buttons in the margin using color. Two variants of this technique, Static Coloring and Dynamic Coloring, were evaluated and compared with the mouse. Dynamic Coloring was found to be the best, with click times much faster than those of other gaze based techniques, and number of incorrect clicks exactly the same as that of the mouse. Some users also preferred Dynamic Coloring over the mouse in the subjective rankings. The results indicate that gaze control certainly has the potential of being used as a mainstream input method.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test14() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Fraud Detection in Online Auctions");
		String[] author = {"Sidney Tsang"};
		tester.setAuthors(author);
		tester.setSupervisors("Dr. Gillian Dobbie");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Science");
		tester.setAbstractx("Online auctions are auctions held over the internet. Online auctions avoid some of the problems of traditional auctions, such as geographical and time restrictions, and limited audiences. However, this has also made them a target for fraud. The majority of previous approaches for reducing auction fraud make use of machine learning techniques to identify suspicious users and auctions. However, many of these approaches have generally encountered the same challenges during implementation and evaluation. These challenges include lack of good quality datasets, imbalance in the number of normal and fraudulent examples, heterogeneity of both normal and fraudulent behaviours, and behaviours which evolve over time. In this thesis, we explore methods of dealing with some of these challenges. To address the lack of good datasets, we implement an agent-based simulation for online auctions as a means of generating synthetic auction data. The simulation models the behaviour of normal bidders and sellers, based on real online auction data. The synthetic data was evaluated using three methods, and results show that the synthetic data is similar to real data. We then applied the simulation to evaluate an existing fraud detection method to show that the simulation can be used as a source of data for evaluating fraud detection algorithms. To address the difficulty in creating detection models for different fraud behaviours and strategies, we demonstrate using supervised learning methods with our simulation to easily create models for detecting arbitrary types of fraud. Models created using this approach were shown to have higher accuracy compared to an existing fraud detection method even after tuning. The caveat is the fraud type of interest must be explicitly defined. To deal with the limitations of the previous approach and to avoid the need for model retraining when fraud behaviours change, we propose an unsupervised method based on anomaly detection. Since the method uses an anomaly detection approach, the model can adapt to changes in user and fraudulent behaviour: the method will identify users who behave differently to the majority of other users. The method makes use of additional network information to identify groups of users that appear suspicious. Extensive evaluation using synthetic data shows that it has higher accuracy than other related approaches. When applied to a real dataset, our method finds a reasonable number of potentially fraudulent users who exhibit unusual characteristics when compared to normal users.");
		fail("Not yet implemented");
	}
	
	@Test
	public void test15() {
		MetadataStorer tester = new MetadataStorer();
		tester.setTitle("Supporting Information Searching in Software Architecture Documents");
		String[] author = {"Moon Ting Su"};
		tester.setAuthors(author);
		tester.setSupervisors("Professor John Hosking and Professor John Grundy");
		tester.setDegree("Doctor of Philosophy");
		tester.setDegreeDiscp("Computer Science ");
		tester.setAbstractx("One of the key problems with Software Architecture Documents (ADs) is the difficulty of finding needed information from them. Most of the existing studies focus on the production aspect of ADs or Architectural Knowledge (AK), to support information finding. This research focuses on the consumption aspect of ADs, in particular on the chunking of architectural information in ADs based on consumers’ usage of ADs, and understanding the foraging behaviour of the consumers. We proposed the notion of chunk to alleviate information searching problem. As a collection of related pieces of architectural information needed for a particular task, a chunk simplifies finding of information by consumers engage with similar tasks, by enabling related architectural information which otherwise may be dispersed in an AD to be retrieved collectively as a unit. We identified chunks by finding ‘commonality’ in the consumers’ usage of the information in ADs when engaged with certain information-seeking task. We collected two types of consumers’ usage data to identify chunks: annotation data (such as ratings, highlighted content, and so on) and interaction data (which captures consumers’ interactions with the pages of a document and elements on the pages). We developed KaitoroCap, an online prototype tool, for creating ADs, collecting annotation data, and interaction data (manifested as consumers’ exploration paths through documents). We found that chunks based on consumers’ usage data (in particular annotation data) of ADs exist. We found that consumers’ ratings of sections show potential in producing satisfactory chunks for the respective task, when the contributing consumers have strong software architecture background and explore the documents in local electronic or printed environment. The goodness of a chunk was determined using criteria which tradeoff the recall and precision measures of the chunk. These measures were calculated by benchmarking against the oracle set for the task. The goodness of a chunk approximates to its support for information searching in the document for similar information-seeking task. We found some differences between industry practitioners and academic professionals in terms of chunks found and information needed. Our investigation into the foraging behaviour of the consumers of ADs showed that: main logical components and use cases are common forages; foraging sequences starting with the overview of a document, main logical components, quality requirements, use cases and external dependencies, support the understanding of the described software architecture; referencing of Table of Contents, exploration based on titles and subtitles, skipping sections, and forward-browsing long sections are popular foraging styles, but backtracking to earlier sections is unpopular; main features of ADs that support understanding of the described software architecture are diagrams, views and design decisions, and the main hindrance is too much text with a lack of diagrams. In conclusion, our research produced some early insights into usage-based chunking of architectural information and architectural information foraging. However, much still needs to be done to gain better insights into supporting information searching in ADs based on their actual consumption by consumers, with many exciting challenges-cum-opportunities waiting to be addressed. ");
		fail("Not yet implemented");
	}
}
